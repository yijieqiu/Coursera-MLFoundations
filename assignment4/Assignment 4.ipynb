{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Question 1](#q1)\n",
    "2. [Question 2](#q2)\n",
    "3. [Question 3-4](#q3-4)\n",
    "4. [Question 5](#q5)\n",
    "5. [Question 6-7](#q6-7)\n",
    "6. [Questions 8-10](#q8-10)\n",
    "7. [Question 11-12](#q11-12)\n",
    "8. [Questions 13-20](#q13-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 <a id=\"q1\" />\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q1.png\" alt=\"Question 1\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: Deterministic noise will increase\n",
    "\n",
    "**Explanation**: Deterministic noise arise from the delta between complexity of target function and that of the model. Thus when model if of lower order compared to target function, deterministic noise tends to increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 <a id=\"q2\" />\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q2.png\" alt=\"Question 2\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\mathcal{H}(10, 3) \\subset \\mathcal{H}(10,4)$\n",
    "\n",
    "**Explanation**: Given definition of the hypothesis, larger value of parameter $d_0$ means the resulting hypothesis will contain more higher-order terms (more non-zero $w_i$), which implies that the hypothesis set is larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3-4  <a id=\"q3-4\"/>\n",
    "#### Question 3\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q3.png\" alt=\"Question 3\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $w(t+1) \\leftarrow (1 + \\frac{2\\eta\\lambda}{N})w(t) -\\eta\\triangledown E_{in}(w(t))$\n",
    "\n",
    "**Explanation**: Recall that gradient descent obtains updated weight $w(t+1)$ by moving step size $\\eta$ in the **opposite** direction of current error gradient $\\triangledown E_{in}(w(t))$. Substitute in the definition of augmented error (note that $w^Tw$ is the matrix form of $w^2$) to obtain the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q4.png\" alt=\"Question 4\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\|w_{reg}(\\lambda)\\| \\leq \\|w_{lin}\\|$ for any $\\lambda > 0$\n",
    "\n",
    "**Explanation**: Recall the optimal solution for unconstrained linear regression is $w_{lin} = (X^TX)^{-1}X^Ty$, whereas the optimal solution with regularization (ridge regession) is $w_{reg} = (X^TX + \\lambda I)^{-1}X^Ty$. Since $\\lambda$ is in the inverse portion, any $\\lambda > 0$ would give $\\|w_{reg}\\| < \\|w_{lin}\\|$, with the special case of $\\|w_{reg}\\| = \\|w_{lin}\\|$ when $\\lambda = 0$ (no regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 <a id=\"q5\"/>\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q5.png\" alt=\"Question 5\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: None of the other choices\n",
    "\n",
    "**Explanation**: See code snippet below. Note that this question **cannot be solved analytically** given the number of unknown variables. The only approach is to produce a set of linear regression models for each value of $\\rho$ and calculate the respective leave-one-out cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-one-out cross-validation error for h0 is 0.5\n",
      "Leave-one-out cross-validation error for h1 when rho = 2.3941701709713277 is 1.135043367685941\n",
      "Leave-one-out cross-validation error for h1 when rho = 0.8555996771673521 is 64.66494840795228\n",
      "Leave-one-out cross-validation error for h1 when rho = 4.335661307243996 is 0.4999999999999998\n",
      "Leave-one-out cross-validation error for h1 when rho = 2.5593964634688433 is 0.9868839293305472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "# When the model is a horizontal line, the intercept must be the average of all y\n",
    "def q5_const_loocv():\n",
    "    y = [0.0, 1.0, 0.0]\n",
    "    loocv = 0\n",
    "    for i in range(3):\n",
    "        # Hack to leave out the sample at index i\n",
    "        y_prime = y[:i] + y[i+1:]\n",
    "        b0 = np.mean(y_prime)\n",
    "        loocv += (b0 - y[i])**2\n",
    "    print(\"Leave-one-out cross-validation error for h0 is {}\".format(loocv/3))\n",
    "\n",
    "def q5_loocv(rho):\n",
    "    x = [-1.0, rho,  1.0]\n",
    "    y = [0.0, 1.0, 0.0]\n",
    "    loocv = 0\n",
    "    for i in range(3):\n",
    "        linreg = LinearRegression(fit_intercept=True)\n",
    "        # Hack to leave out the sample at index i\n",
    "        x_prime = np.array(x[:i] + x[i+1:])\n",
    "        y_prime = np.array(y[:i] + y[i+1:])\n",
    "        linreg.fit(x_prime[:, np.newaxis], y_prime)\n",
    "        a1 = linreg.coef_[0]\n",
    "        b1 = linreg.intercept_\n",
    "        loocv += (a1 * x[i] + b1 - y[i])**2 \n",
    "    print(\"Leave-one-out cross-validation error for h1 when rho = {} is {}\".format(rho, loocv/3))\n",
    "\n",
    "def q5():\n",
    "    q5_const_loocv()\n",
    "    rho1 = math.sqrt(4 + math.sqrt(3))\n",
    "    rho2 = math.sqrt(math.sqrt(3) - 1)\n",
    "    rho3 = math.sqrt(9 + 4 * math.sqrt(6))\n",
    "    rho4 = math.sqrt(9 - math.sqrt(6))\n",
    "    for rho in [rho1, rho2, rho3, rho4]:\n",
    "        q5_loocv(rho)\n",
    "\n",
    "q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6-7 <a id=\"q6-7\"/>\n",
    "#### Question 6\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q6.png\" alt=\"Question 6\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: To make sure that at least one person receives correct predictions on all 55 games from the sender, after the first letter `predicts' the outcome of the first game, the sender should target at least 1616 people with the second letter.\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "  * Total of  $2^5 = 32$ combinations for 5 games, thus the sender needs to start with 32 letters to ensure at least one person receives the correct prediction on all 5 games.\n",
    "  * For each subsequent game, the sender only needs to target half of the receivers from the previous game (who received the correct prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q7.png\" alt=\"Question 7\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: 370\n",
    "\n",
    "**Explanation**: 1000 - (32 + 16 + 8 + 4 + 2 + 1) * 10 = 370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8-10 <a id=\"q8-10\"/>\n",
    "#### Question 8\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q8.png\" alt=\"Question 8\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: 1\n",
    "\n",
    "**Explanation**: The hypothesis is mathematically derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q9.png\" alt=\"Question 9\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: 0.271\n",
    "\n",
    "**Explanation**: Finite-bin Hoeffding's inequality for Bernoulli distribution is $ \\mathbb{P}(|S_n - \\mathbb{E}[S_n]| \\ge \\epsilon)) \\leq 2Me^{-2n\\epsilon^2}$, substituting in $N = 10,000, M = 1$ and $\\epsilon = 0.01$ gives:\n",
    "$$ P = 2 \\exp^{-2*10000*0.01*0.01} = 0.271$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q10.png\" alt=\"Question 10\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $a(x) \\text{ AND } g(x)$\n",
    "\n",
    "**Explanation**: Training data used to learn $g(x)$ had alredy been \"pre-filtered\" by $a(x)$ to only include the approved cases. Therefore the two models must be used in conjunction to provide satisfactory result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11-12 <a id=\"q11-12\"/>\n",
    "#### Question 11\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q11.png\" alt=\"Question 11\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $(X^TX + \\bar{X}^T\\bar{X})^{-1}(X^Ty + \\bar{X}^T\\bar{y})$\n",
    "\n",
    "**Explanation**: Extend the optimal solution of linear regression to include the virtual samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q12.png\" alt=\"Question 12\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\bar{X} = \\sqrt{\\lambda}X, \\bar{y} = y$\n",
    "\n",
    "**Explanation**: Recall optimal solution of ridge regression:\n",
    "        $$w_{reg} = (X^TX + \\lambda I)^{-1}X^T y$$\n",
    "Let:\n",
    "    $$(X^TX + \\bar{X}^T\\bar{X})^{-1}(X^Ty + \\bar{X}^T\\bar{y}) = (X^TX + \\lambda I)^{-1}X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13-20 <a id=\"q13-20\"/>\n",
    "#### Question 13\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q13.png\" alt=\"Question 13\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $E_{in} = 0.050, E_{out} = 0.045$\n",
    "\n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions 13-20 helper methods\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from file under current directory and parse into X, Y arrays, assuming Y is the last column of input\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(filename)\n",
    "    col, row = data.shape\n",
    "    X = np.c_[np.ones((col, 1)), data[:, 0: row - 1]]\n",
    "    Y = data[:, row - 1:row]\n",
    "    return X, Y\n",
    "\n",
    "def error(X, Y, w):\n",
    "    \"\"\"\n",
    "    Calculate 0/1 error for given inputs. Note that data used in this assignment use 1/-1 for Y instead of 0/1,\n",
    "    the error measure calculation has thus been adapted accordingly.\n",
    "    \n",
    "    Parameters:\n",
    "    X (ndarray): feature matrix\n",
    "    Y (ndarray): target matrix\n",
    "    w (ndarray): weights obtained from regularized linear regression\n",
    "    \"\"\"\n",
    "    y_hat = np.sign(X.dot(w))\n",
    "    y_hat[y_hat == 0] = -1\n",
    "    return np.sum(y_hat != Y) / len(Y)\n",
    "\n",
    "def ridge_reg(X, Y, lam):\n",
    "    \"\"\"\n",
    "    Perform regularized linear regression (ridge regression) on the given data, with given Lagrange multiplier\n",
    "    \n",
    "    Parameters:\n",
    "    X (ndarray): feature matrix\n",
    "    Y (ndarray): target matrix\n",
    "    lam (int): Lagrange multiplier\n",
    "    \"\"\"\n",
    "    row, col = X.shape\n",
    "    w = linalg.pinv(X.T.dot(X) + lam * np.eye(col)).dot(X.T).dot(Y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_reg = [[-0.93238149]\n",
      " [ 1.04618645]\n",
      " [ 1.046171  ]]\n",
      "E_in = 0.05, E_out = 0.045\n"
     ]
    }
   ],
   "source": [
    "# Question 13\n",
    "def q13():\n",
    "    X, Y = load_data('hw4_train.dat')\n",
    "    X_test, Y_test = load_data('hw4_test.dat')\n",
    "    w_reg = ridge_reg(X, Y, 10)\n",
    "    print('w_reg = {}'.format(w_reg))\n",
    "    e_in = error(X, Y, w_reg)\n",
    "    e_out = error(X_test, Y_test, w_reg)\n",
    "    print('E_in = {}, E_out = {}'.format(e_in, e_out))\n",
    "\n",
    "q13()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q14.png\" alt=\"Question `4\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\log_{10}\\lambda = -8, E_{in} = 0.015, E_{out} = 0.020$\n",
    "\n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q15.png\" alt=\"Question 15\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\log_{10}\\lambda = -7, E_{in} = 0.030, E_{out} = 0.015$\n",
    "\n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For question 14, choose log10_lambda with smallest E_in\n",
      "log10_lambda = -8, E_in = 0.015, E_out = 0.02\n",
      "For question 15, choose log10_lambda with smallest E_out\n",
      "log10_lambda = -7, E_in = 0.03, E_out = 0.015\n"
     ]
    }
   ],
   "source": [
    "# Question 14/15 common\n",
    "def q14_15_common(in_sample_err):\n",
    "    \"\"\"\n",
    "    Helper method shared between question 14 and 15. Chooses the log10 Lagrange multiplier that gives either the\n",
    "    smallest E_in, or smallest E_out\n",
    "    \n",
    "    Parameters:\n",
    "    in_sample_err (bool): True if evaluate based on E_in, False if evaluate based on E_out\n",
    "    \"\"\"\n",
    "    X, Y = load_data('hw4_train.dat')\n",
    "    X_test, Y_test = load_data('hw4_test.dat')\n",
    "    lams = range(2, -11, -1)\n",
    "    errors_in = []\n",
    "    errors_out = []\n",
    "    for lam in lams:\n",
    "        w_reg = ridge_reg(X, Y, 10**lam)\n",
    "        e_in = error(X, Y, w_reg)\n",
    "        e_out = error(X_test, Y_test, w_reg)\n",
    "        errors_in.append(e_in)\n",
    "        errors_out.append(e_out)\n",
    "        \n",
    "    # Find index of smallest error\n",
    "    if in_sample_err:\n",
    "        idx = errors_in.index(min(errors_in))\n",
    "    else:\n",
    "        idx = errors_out.index(min(errors_out))\n",
    "    print('log10_lambda = {}, E_in = {}, E_out = {}'.format(lams[idx], errors_in[idx], errors_out[idx]))\n",
    "\n",
    "\n",
    "# Question 14\n",
    "print('For question 14, choose log10_lambda with smallest E_in')\n",
    "q14_15_common(True)\n",
    "\n",
    "# Question 15\n",
    "print('For question 15, choose log10_lambda with smallest E_out')\n",
    "q14_15_common(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q16.png\" alt=\"Question 16\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\log_{10}\\lambda = -8, E_{train} = 0, E_{val} = 0.050. E_{out} = 0.025$\n",
    "\n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q17.png\" alt=\"Question 17\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\log_{10}\\lambda = 0, E_{train} = 0.033, E_{val} = 0.038. E_{out} = 0.028$\n",
    "\n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 18\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q18.png\" alt=\"Question 18\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $E_{in} = 0.035, E_{out} = 0.020$\n",
    "\n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For question 16, choose log10_lambda with smallest E_train\n",
      "log10_lambda = -8, E_train = 0.0, E_val = 0.05, E_out = 0.025\n",
      "For question 17, choose log10_lambda with smallelst E_val\n",
      "log10_lambda = 0, E_train = 0.03333333333333333, E_val = 0.0375, E_out = 0.028\n"
     ]
    }
   ],
   "source": [
    "# Question 16/17 common \n",
    "def q16_17_common(training_err):\n",
    "    \"\"\"\n",
    "    Helper method shared between question 16 and 17.\n",
    "    1. Performs an in-order 120/80 split on full training data to obtain training and validation set\n",
    "    2. Trains ridge regression model based on training set and obtains w_reg for given Lagrange multiplier\n",
    "    3. Chooses the log10 Lagrange multiplier that gives either the smallest E_train, or smallest E_val\n",
    "    \n",
    "    Parameters:\n",
    "    training_err (bool): True if evaluate based on E_train, False if evaluate based on E_val\n",
    "    \"\"\"\n",
    "    X, Y = load_data('hw4_train.dat')\n",
    "    # Split training data into training and validation sets\n",
    "    X_tr = X[0:120, :]\n",
    "    Y_tr = Y[0:120, :]\n",
    "    X_val = X[120:, :]\n",
    "    Y_val = Y[120:, :]\n",
    "    \n",
    "    X_test, Y_test = load_data('hw4_test.dat')\n",
    "    \n",
    "    lams = range(2, -11, -1)\n",
    "    errors_tr = []\n",
    "    errors_val = []\n",
    "    errors_out = []\n",
    "    for lam in lams:\n",
    "        w_reg = ridge_reg(X_tr, Y_tr, 10**lam)\n",
    "        e_tr = error(X_tr, Y_tr, w_reg)\n",
    "        e_val = error(X_val, Y_val, w_reg)\n",
    "        e_out = error(X_test, Y_test, w_reg)\n",
    "        errors_tr.append(e_tr)\n",
    "        errors_val.append(e_val)\n",
    "        errors_out.append(e_out)\n",
    "        \n",
    "    # Find index of smallest error\n",
    "    if training_err:\n",
    "        idx = errors_tr.index(min(errors_tr))\n",
    "    else:\n",
    "        idx = errors_val.index(min(errors_val))\n",
    "        \n",
    "    print('log10_lambda = {}, E_train = {}, E_val = {}, E_out = {}'\n",
    "          .format(lams[idx], errors_tr[idx], errors_val[idx], errors_out[idx]))    \n",
    "    \n",
    "\n",
    "# Question 16\n",
    "print('For question 16, choose log10_lambda with smallest E_train')\n",
    "q16_17_common(True)\n",
    "\n",
    "# Question 17\n",
    "print('For question 17, choose log10_lambda with smallelst E_val')\n",
    "q16_17_common(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in = 0.035, E_out = 0.02\n"
     ]
    }
   ],
   "source": [
    "# Question 18\n",
    "def q18():\n",
    "    X, Y = load_data('hw4_train.dat')\n",
    "    X_test, Y_test = load_data('hw4_test.dat')\n",
    "    # Best lambda found in Q17\n",
    "    lam = 0\n",
    "    \n",
    "    w_reg = ridge_reg(X, Y, 10**lam)\n",
    "    e_in = error(X, Y, w_reg)\n",
    "    e_out = error(X_test, Y_test, w_reg)\n",
    "    print('E_in = {}, E_out = {}'.format(e_in, e_out))\n",
    "    \n",
    "q18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 19\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q19.png\" alt=\"Question 19\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\log_{10}\\lambda = -8, E_{val} = 0.03$\n",
    "    \n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 20\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q20.png\" alt=\"Question 20\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $E_{in} = 0.015, E_{out} = 0.02$\n",
    "    \n",
    "**Explanation**: See code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10_lambda = -8, E_cv = 0.0375\n"
     ]
    }
   ],
   "source": [
    "def k_fold(X, Y, lam, n=40):\n",
    "    \"\"\"\n",
    "    Perform K-fold cross validation for ridge regression models training from given X, Y, and Lagrange multiplier\n",
    "    \n",
    "    Parameters:\n",
    "    X (ndarray): feature matrix\n",
    "    Y (ndarray): target matrix\n",
    "    lam (int): lagrange multiplier (log10)\n",
    "    n (int): number of data points in each fold, default is 40\n",
    "    \"\"\"\n",
    "    # Indexes for the split\n",
    "    idx = list(range(0, len(X), n))\n",
    "    e_val = 0\n",
    "    for i in idx:\n",
    "        j = i + n\n",
    "        # Split input data into k folds of size n. Use each fold as validation set, and other folds for training\n",
    "        X_val = X[i:j, :]\n",
    "        X_tr = np.r_[X[0:i, :], X[j:, :]]\n",
    "        Y_val = Y[i:j, :]\n",
    "        Y_tr = np.r_[Y[0:i, :], Y[j:, :]]\n",
    "        w_reg = ridge_reg(X_tr, Y_tr, 10**lam)\n",
    "        e_val += error(X_val, Y_val, w_reg)\n",
    "        \n",
    "    return e_val / (len(idx) - 1)\n",
    "    \n",
    "def q19():\n",
    "    X, Y = load_data('hw4_train.dat')\n",
    "    lams = range(2, -11, -1)\n",
    "    errors_cv = []\n",
    "    for lam in lams:\n",
    "        e_cv = k_fold(X, Y, lam)\n",
    "        errors_cv.append(e_cv)\n",
    "    \n",
    "    # Find index of smallest error\n",
    "    idx = errors_cv.index(min(errors_cv))\n",
    "    print('log10_lambda = {}, E_cv = {}'.format(lams[idx], errors_cv[idx]))\n",
    "    \n",
    "    \n",
    "q19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in = 0.015, E_out = 0.02\n"
     ]
    }
   ],
   "source": [
    "def q20():\n",
    "    X, Y = load_data('hw4_train.dat')\n",
    "    X_test, Y_test = load_data('hw4_test.dat')\n",
    "    # Best lambda found in Q19\n",
    "    lam = -8\n",
    "    w_reg =ridge_reg(X, Y, 10**lam)\n",
    "    e_in = error(X, Y, w_reg)\n",
    "    e_out = error(X_test, Y_test, w_reg)\n",
    "    print('E_in = {}, E_out = {}'.format(e_in, e_out))\n",
    "    \n",
    "q20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

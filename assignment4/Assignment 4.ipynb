{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Question 1](#q1)\n",
    "2. [Question 2](#q2)\n",
    "3. [Question 3-4](#q3-4)\n",
    "4. [Question 5](#q5)\n",
    "5. [Question 6-7](#q6-7)\n",
    "6. [Questions 8-10](#q8-10)\n",
    "7. [Question 11-12](#q11-12)\n",
    "8. [Questions 13-18](#q13-18)\n",
    "9. [Questions 19-20](#q19-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 <a id=\"q1\" />\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q1.png\" alt=\"Question 1\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: Deterministic noise will increase\n",
    "\n",
    "**Explanation**: Deterministic noise arise from the delta between complexity of target function and that of the model. Thus when model if of lower order compared to target function, deterministic noise tends to increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 <a id=\"q2\" />\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q2.png\" alt=\"Question 2\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\mathcal{H}(10, 3) \\subset \\mathcal{H}(10,4)$\n",
    "\n",
    "**Explanation**: Given definition of the hypothesis, larger value of parameter $d_0$ means the resulting hypothesis will contain more higher-order terms (more non-zero $w_i$), which implies that the hypothesis set is larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3-4  <a id=\"q3-4\"/>\n",
    "#### Question 3\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q3.png\" alt=\"Question 3\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $w(t+1) \\leftarrow (1 + \\frac{2\\eta\\lambda}{N})w(t) -\\eta\\triangledown E_{in}(w(t))$\n",
    "\n",
    "**Explanation**: Recall that gradient descent obtains updated weight $w(t+1)$ by moving step size $\\eta$ in the **opposite** direction of current error gradient $\\triangledown E_{in}(w(t))$. Substitute in the definition of augmented error (note that $w^Tw$ is the matrix form of $w^2$) to obtain the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q4.png\" alt=\"Question 4\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\|w_{reg}(\\lambda)\\| \\leq \\|w_{lin}\\|$ for any $\\lambda > 0$\n",
    "\n",
    "**Explanation**: Recall the optimal solution for unconstrained linear regression is $w_{lin} = (X^TX)^{-1}X^Ty$, whereas the optimal solution with regularization (ridge regession) is $w_{reg} = (X^TX + \\lambda I)^{-1}X^Ty$. Since $\\lambda$ is in the inverse portion, any $\\lambda > 0$ would give $\\|w_{reg}\\| < \\|w_{lin}\\|$, with the special case of $\\|w_{reg}\\| = \\|w_{lin}\\|$ when $\\lambda = 0$ (no regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 <a id=\"q5\"/>\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q5.png\" alt=\"Question 5\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: None of the other choices\n",
    "\n",
    "**Explanation**: See code snippet below. Note that this question **cannot be solved analytically** given the number of unknown variables. The only approach is to produce a set of linear regression models for each value of $\\rho$ and calculate the respective leave-one-out cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave-one-out cross-validation error for h0 is 0.5\n",
      "Leave-one-out cross-validation error for h1 when rho = 2.3941701709713277 is 1.135043367685941\n",
      "Leave-one-out cross-validation error for h1 when rho = 0.8555996771673521 is 64.66494840795228\n",
      "Leave-one-out cross-validation error for h1 when rho = 4.335661307243996 is 0.4999999999999998\n",
      "Leave-one-out cross-validation error for h1 when rho = 2.5593964634688433 is 0.9868839293305472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "\n",
    "# When the model is a horizontal line, the intercept must be the average of all y\n",
    "def q5_const_loocv():\n",
    "    y = [0.0, 1.0, 0.0]\n",
    "    loocv = 0\n",
    "    for i in range(3):\n",
    "        # Hack to leave out the sample at index i\n",
    "        y_prime = y[:i] + y[i+1:]\n",
    "        b0 = np.mean(y_prime)\n",
    "        loocv += (b0 - y[i])**2\n",
    "    print(\"Leave-one-out cross-validation error for h0 is {}\".format(loocv/3))\n",
    "\n",
    "def q5_loocv(rho):\n",
    "    x = [-1.0, rho,  1.0]\n",
    "    y = [0.0, 1.0, 0.0]\n",
    "    loocv = 0\n",
    "    for i in range(3):\n",
    "        linreg = LinearRegression(fit_intercept=True)\n",
    "        # Hack to leave out the sample at index i\n",
    "        x_prime = np.array(x[:i] + x[i+1:])\n",
    "        y_prime = np.array(y[:i] + y[i+1:])\n",
    "        linreg.fit(x_prime[:, np.newaxis], y_prime)\n",
    "        a1 = linreg.coef_[0]\n",
    "        b1 = linreg.intercept_\n",
    "        loocv += (a1 * x[i] + b1 - y[i])**2 \n",
    "    print(\"Leave-one-out cross-validation error for h1 when rho = {} is {}\".format(rho, loocv/3))\n",
    "\n",
    "def q5():\n",
    "    q5_const_loocv()\n",
    "    rho1 = math.sqrt(4 + math.sqrt(3))\n",
    "    rho2 = math.sqrt(math.sqrt(3) - 1)\n",
    "    rho3 = math.sqrt(9 + 4 * math.sqrt(6))\n",
    "    rho4 = math.sqrt(9 - math.sqrt(6))\n",
    "    for rho in [rho1, rho2, rho3, rho4]:\n",
    "        q5_loocv(rho)\n",
    "\n",
    "q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6-7 <a id=\"q6-7\"/>\n",
    "#### Question 6\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q6.png\" alt=\"Question 6\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: To make sure that at least one person receives correct predictions on all 55 games from the sender, after the first letter `predicts' the outcome of the first game, the sender should target at least 1616 people with the second letter.\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "  * Total of  $2^5 = 32$ combinations for 5 games, thus the sender needs to start with 32 letters to ensure at least one person receives the correct prediction on all 5 games.\n",
    "  * For each subsequent game, the sender only needs to target half of the receivers from the previous game (who received the correct prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q7.png\" alt=\"Question 7\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: 370\n",
    "\n",
    "**Explanation**: 1000 - (32 + 16 + 8 + 4 + 2 + 1) * 10 = 370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8-10 <a id=\"q8-10\"/>\n",
    "#### Question 8\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q8.png\" alt=\"Question 8\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: 1\n",
    "\n",
    "**Explanation**: The hypothesis is mathematically derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q9.png\" alt=\"Question 9\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: 0.271\n",
    "\n",
    "**Explanation**: Finite-bin Hoeffding's inequality for Bernoulli distribution is $ \\mathbb{P}(|S_n - \\mathbb{E}[S_n]| \\ge \\epsilon)) \\leq 2Me^{-2n\\epsilon^2}$, substituting in $N = 10,000, M = 1$ and $\\epsilon = 0.01$ gives:\n",
    "$$ P = 2 \\exp^{-2*10000*0.01*0.01} = 0.271$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q10.png\" alt=\"Question 10\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $a(x) \\text{ AND } g(x)$\n",
    "\n",
    "**Explanation**: Training data used to learn $g(x)$ had alredy been \"pre-filtered\" by $a(x)$ to only include the approved cases. Therefore the two models must be used in conjunction to provide satisfactory result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11-12 <a id=\"q11-12\"/>\n",
    "#### Question 11\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q11.png\" alt=\"Question 11\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $(X^TX + \\bar{X}^T\\bar{X})^{-1}(X^Ty + \\bar{X}^T\\bar{y})$\n",
    "\n",
    "**Explanation**: Extend the optimal solution of linear regression to include the virtual samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "<img src=\"https://github.com/yijieqiu/coursera-ml-foundations/raw/master/assignment4/q12.png\" alt=\"Question 12\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Answer**: $\\bar{X} = \\sqrt{\\lambda}X, \\bar{y} = y$\n",
    "\n",
    "**Explanation**: Recall optimal solution of ridge regression:\n",
    "        $$w_{reg} = (X^TX + \\lambda I)^{-1}X^T y$$\n",
    "Let:\n",
    "    $$(X^TX + \\bar{X}^T\\bar{X})^{-1}(X^Ty + \\bar{X}^T\\bar{y}) = (X^TX + \\lambda I)^{-1}X^T y$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
